# -*- coding: utf-8 -*-
"""project_wikidumps.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KL-1r6wJqUev4yqa4kV9Ea70iNMsUDqC
"""

import sys
from collections import Counter, OrderedDict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import bs4
import nltk
# from nltk.stem.porter import *
# from nltk.corpus import stopwords
from nltk.corpus import stopwords
import requests

nltk.download('stopwords')
from time import time
from timeit import timeit
from pathlib import Path
import pickle
import pandas as pd
import numpy as np
from google.cloud import storage
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict, Counter
import re
import pickle
import numpy as np
import json
import math
import gzip
import io
import math
from itertools import chain
import time
from inverted_index_gcp import InvertedIndex
from logging import exception
import subprocess

subprocess.run(['pip', 'install', 'gcsfs'])
import pickle
from google.cloud import storage
import gcsfs


'''
tokenize and preprocess for the query
'''


RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)
stopwords_frozen = frozenset(stopwords.words('english'))
corpus_stopwords = ["category", "references", "also", "external", "links",
                    "may", "first", "see", "history", "people", "one", "two",
                    "part", "thumb", "including", "second", "following",
                    "many", "however", "would", "became"]

all_stopwords = stopwords_frozen.union(corpus_stopwords)


def tokenize(text):
    """
    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.

    Parameters:
    -----------
    text: string , represting the text to tokenize.

    Returns:
    -----------
    list of tokens (e.g., list of tokens).
    """
    try:
        list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if
                          token.group() not in all_stopwords]
        return list_of_tokens
    except exception:
        print(exception)


def query_preprocessing(query):
    token_query = tokenize(query)
    final_query = token_query  # + Word2Vec(token_query)
    return final_query


'''
load all idx from buckets
'''
file_name = 'michal.pickle'
fs = gcsfs.GCSFileSystem()

with fs.open(f'gs://m_f_anchor/anchor_index.pkl', 'rb') as f:
    # Load the pickle file
    idx_links = pickle.load(f)
print("links_index")

with fs.open(f'gs://m_f_title/title_index/title_index.pkl', 'rb') as f:
    # Load the pickle file
    idx_title = pickle.load(f)

print("title_index")
#
with fs.open(f'gs://m_f_body/body_index.pkl', 'rb') as f:
    # Load the pickle file
    idx_body = pickle.load(f)
print("body_index")

idx_body.title_dict = idx_title.title_dict

'''
  for search_body
  '''

def get_candidate_documents_and_scores(query_to_search, index, bucket_name):
    """
    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search
    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.
    Then it will populate the dictionary 'candidates.'
    For calculation of IDF, use log with base 10.
    tf will be normalized based on the length of the document.

    """
    candidates = {}
    for term in np.unique(query_to_search):
        if term in index.posting_locs:
            list_of_doc = index.read_posting_list(index, term, bucket_name)
            list_of_doc = [(tup[0], tup[1]) for tup in list_of_doc if tup[1] > (50 * len(query_to_search))]
            normlized_tfidf = []

            for doc_id, freq in list_of_doc:
                if (doc_id, freq) == (0, 0):
                    continue
                normlized_tfidf += [(doc_id, (freq / index.DL[doc_id]) * math.log(len(index.DL) / index.df[term], 10) *
                                     dict(Counter(query_to_search))[term])]
            for doc_id, tfidf in normlized_tfidf:
                candidates[(doc_id, term)] = candidates.get((doc_id, term), 0) + tfidf
    return candidates


def fast_cosine_similarity(query, index, bucket_name):
    dict_cosine_sim = {}
    candidates_docs = get_candidate_documents_and_scores(query, index, bucket_name)
    for doc_id_term, scores in candidates_docs.items():
        normalized_score_query = (len(query) * index.DL[doc_id_term[0]])
        dict_cosine_sim[doc_id_term[0]] = scores / normalized_score_query
    return dict_cosine_sim


def get_title_from_wiki(doc_id):
    url = "https://en.wikipedia.org/?curid="
    url += str(doc_id)
    response = requests.get(url=url, )
    soup = bs4.BeautifulSoup(response.content, 'html.parser')
    title = soup.find(id="firstHeading")
    return (doc_id, str(title.text))


def get_top_n(sim_dict, N=100):
    """
    Sort and return the highest N documents according to the cosine similarity score.
    Generate a dictionary of cosine similarity scores

    """
    return sorted([(doc_id, score) for doc_id, score in sim_dict.items()], key=lambda x: x[1], reverse=True)[:N]


def get_topN_score_for_queries(query_to_search, index, bucket_name, N=100):
    """
    Generate a dictionary that gathers for every query its topN score.

    Parameters:
    -----------
    queries_to_search: a dictionary of queries as follows:
                                                        key: query_id
                                                        value: list of tokens.
    index:           inverted index loaded from the corresponding files.
    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function.

    Returns:
    -----------
    return: a dictionary of queries and topN pairs as follows:
                                                        key: query_id
                                                        value: list of pairs in the following format:(doc_id, score).
    """
    cos_sim_dict = fast_cosine_similarity(query_to_search, index, bucket_name)
    list_tuple_topN = get_top_n(cos_sim_dict, N)
    list_topN = [(tup[0], index.title_dict[tup[0]]) for tup in list_tuple_topN]
    return list_topN


# links calculation
from collections import defaultdict
from collections import Counter

'''
for the search_title and search_anchor
'''

def get_topN_title_score_for_queries_title_links(query, index, bucket_name):
    grouped = defaultdict(int)
    candidates = []
    dict_words = {}
    llst = []
    counts = Counter()
    for term in np.unique(query):
        if term in index.posting_locs:  # if term in words:
            list_of_doc = index.read_posting_list(index, term, bucket_name)  # pls[words.index(term)]
            # if list_of_doc != []:
            #     print("list of docs not []")
            candidates += list_of_doc
    for tup in candidates:
        if tup[0] not in dict_words:
            dict_words[tup[0]] = 1
        else:
            dict_words[tup[0]] += 1
    sorted_d = sorted(dict_words.items(), key=lambda x: x[1], reverse=True)
    print(len(sorted_d))
    # return sorted_d
    list_topN = []
    for tup in sorted_d:
        if tup[0] in idx_title.title_dict.keys():
            list_topN.append((tup[0], idx_title.title_dict[tup[0]]))
        else:
            list_topN.append(get_title_from_wiki(tup[0]))
    print(len(list_topN))
    # print(idx_title.title_dict[29784214])
    return list_topN


'''
for the main search
'''


def get_topN_title_score_search(query, index, bucket_name):
    grouped = defaultdict(int)
    candidates = []
    dict_words = {}
    for term in np.unique(query):
        if term in index.posting_locs:  # if term in words:
            list_of_doc = index.read_posting_list(index, term, bucket_name)  # pls[words.index(term)]
            if list_of_doc != []:
                print("list of docs not []")
            candidates += list_of_doc
    for tup in candidates:
        if tup[0] not in dict_words:
            dict_words[tup[0]] = 1
        else:
            dict_words[tup[0]] += 1
    sorted_d = sorted(dict_words.items(), key=lambda x: x[1], reverse=True)
    return sorted_d


import math


class BM25_from_index:

    def __init__(self, index, bucket_name, k1=1.5, b=0.75):
        self.b = b
        self.k1 = k1
        self.index = index
        self.N = len(index.DL)
        self.AVGDL = np.sum(list(index.DL.values())) / self.N
        self.bucket_name = bucket_name

    def calc_idf(self, list_of_tokens):
        idf = {}
        for term in list_of_tokens:
            if term in self.index.df.keys():
                n_ti = self.index.df[term]
                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))
            else:
                pass
        return idf

    def search(self, query, N=100):
        tokens = query
        self.idf = self.calc_idf(tokens)
        term_frequencies = {}
        for token in tokens:
            if token in self.index.df:
                try:
                    # save posting list as dictionary
                    term_frequencies[token] = dict(InvertedIndex.read_posting_list(self.index, token, self.bucket_name))
                except:
                    continue
        candidates = get_candidate_bm25(tokens, self.index, self.bucket_name)
        scores = sorted([(doc_id, self._score(tokens, doc_id, term_frequencies)) for doc_id in candidates],
                        key=lambda x: x[1], reverse=True)[:N]

        return scores

    def _score(self, query, doc_id, term_frequencies):
        score = 0.0
        if doc_id == 0:
            return score
        doc_len = self.index.DL[doc_id]
        for term in query:
            if doc_id in term_frequencies[term]:
                # calculate BM25 value
                freq = term_frequencies[term][doc_id]
                numerator = self.idf[term] * freq * (self.k1 + 1)
                denominator = freq + self.k1 * (1 - self.b + (self.b * doc_len / self.AVGDL))
                try:
                    score += (numerator / denominator)
                except:
                    score += (numerator / denominator)

        return score


def get_candidate_bm25(query_to_search, index, bucket_name):
    candidates = []
    for term in np.unique(query_to_search):
        try:
            pls = InvertedIndex.read_posting_list(index, term, bucket_name)  # get posting list of the term if exists
        except:
            continue
        candidates += pls
    dict_scores = {}
    for doc_id, tf in pls:
        if doc_id in dict_scores:
            dict_scores[doc_id] += tf
        else:
            dict_scores[doc_id] = tf

    return np.unique([doc_id for doc_id, tf in dict_scores.items()])


def search(query, bucket_body, bucket_title, bucket_anchor, N=100):
    # lst_query = [query]
    if len(query) <= 2:
        # title_scores = get_topN_title_score_search(query, idx_title, bucket_title)
        # print(title_scores)
        anchor_scores = get_topN_title_score_search(query, idx_links, bucket_anchor)
        list_topN = []
        for tup in anchor_scores:
            if tup[0] in idx_title.title_dict.keys():
                list_topN.append((tup[0], idx_title.title_dict[tup[0]]))
            else:
                list_topN.append(get_title_from_wiki(tup[0]))
        return list_topN[0:N]

    else:
        bm25_body = BM25_from_index(idx_body, bucket_body)  # create BM25 object
        list_topN = []
        title_scores = list(bm25_body.search(query))
        for tup in title_scores:
            if tup[0] in idx_title.title_dict.keys():
                list_topN.append((int(tup[0]), idx_title.title_dict[tup[0]]))
            else:
                list_topN.append(get_title_from_wiki(int(tup[0])))
        print(type(list_topN))
        return list_topN[:N]


def page_rank_for_bm25(doc_id):
    return dict_page_rank[doc_id]


'''
for search_pagerank
'''

file_name_page_rank = 'pr/part-00000-c0f8e37a-8c0c-4647-9831-873b34473841-c000.csv.gz'
# Connect to the bucket
storage_client = storage.Client()
bucket = storage_client.bucket("bucket_m_f")
# Load the file
blob = bucket.blob(file_name_page_rank)
content = blob.download_as_string()
df = pd.read_csv(io.BytesIO(content), compression='gzip', names=['id', 'score'])
# Convert the DataFrame to a dictionary with column labels as keys
dict_page_rank = df.to_dict(orient='dict')



def page_rank_helper(id_list):
    tuple_list = [(k, v) for k, v in dict_page_rank['id'].items() if k in id_list]
    return tuple_list

'''
for search_pageview
'''

fs = gcsfs.GCSFileSystem()
with fs.open(f'gs://bucket_m_f/file_view.pkl', 'rb') as f:
    # Load the pickle file
    data = pickle.load(f)


def get_pageview(list_ids):
    return [data[id] for id in list_ids]
